# gpt-from-scratch

This repository contains an implementation of a simplified Generative Pre-trained Transformer (GPT) model, built entirely from scratch. The goal of this project is to understand the foundational concepts of GPT, including tokenization, transformer architecture, attention mechanisms, and model training.